"""
Q-Learning Demo - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Q-learning
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏ò‡∏¥‡∏ï

‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
1. ‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ù‡∏∂‡∏Å Q-learning agent
2. ‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞ visualizations ‡∏ï‡πà‡∏≤‡∏á‡πÜ

‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á numpy ‡πÅ‡∏•‡∏∞ matplotlib ‡∏Å‡πà‡∏≠‡∏ô
pip install numpy matplotlib
"""

import sys
import os

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö import modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    import numpy as np
    import matplotlib.pyplot as plt
    from grid_world import GridWorld
    from q_learning import QLearningAgent
    HAS_PACKAGES = True
except ImportError as e:
    print(f"Warning: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ import packages ‡πÑ‡∏î‡πâ: {e}")
    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install numpy matplotlib")
    HAS_PACKAGES = False

def run_basic_demo():
    """‡∏£‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Q-learning (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ numpy/matplotlib)"""
    print("=== Q-Learning Demo (Basic Version) ===")
    print("‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á Q-learning ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ")
    print()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á simple grid world 3x3
    grid_size = 3
    print(f"Grid World ‡∏Ç‡∏ô‡∏≤‡∏î {grid_size}x{grid_size}:")
    print("S = Start (0,0)")
    print("G = Goal (2,2)")
    print("X = Obstacle (1,1)")
    print()
    
    # ‡πÅ‡∏™‡∏î‡∏á grid
    for i in range(grid_size):
        row_str = ""
        for j in range(grid_size):
            if (i, j) == (0, 0):
                row_str += "S "
            elif (i, j) == (2, 2):
                row_str += "G "
            elif (i, j) == (1, 1):
                row_str += "X "
            else:
                row_str += ". "
        print(row_str)
    print()
    
    # ‡∏à‡∏≥‡∏•‡∏≠‡∏á Q-learning process
    print("‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Q-learning:")
    print("- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Q-values = 0 ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print("- ‡πÉ‡∏ä‡πâ epsilon-greedy policy (Œµ = 0.1)")
    print("- Learning rate Œ± = 0.1")
    print("- Discount factor Œ≥ = 0.9")
    print()
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-learning update
    print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Q-learning update:")
    print("‡∏´‡∏≤‡∏Å agent ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà (0,0) ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action 'right'")
    print("‡πÑ‡∏õ‡∏¢‡∏±‡∏á state (0,1) ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ reward = -0.1")
    print()
    print("Q-learning formula:")
    print("Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]")
    print()
    print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:")
    print("Q(0,right) ‚Üê 0 + 0.1 √ó [-0.1 + 0.9 √ó 0 - 0]")
    print("Q(0,right) ‚Üê 0 + 0.1 √ó (-0.1)")
    print("Q(0,right) ‚Üê -0.01")
    print()
    
    print("‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ù‡∏∂‡∏Å 1000 episodes:")
    print("Agent ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢")
    print("‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ")

def run_full_demo():
    """‡∏£‡∏±‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏° visualizations"""
    print("=== Q-Learning Demo (Full Version) ===")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á environment
    env = GridWorld(size=5)
    print(f"‡∏™‡∏£‡πâ‡∏≤‡∏á Grid World ‡∏Ç‡∏ô‡∏≤‡∏î {env.size}x{env.size}")
    print(f"Start: {env.start_pos}, Goal: {env.goal_pos}")
    print(f"Obstacles: {env.obstacles}")
    print()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á agent
    agent = QLearningAgent(
        n_states=env.n_states,
        n_actions=env.n_actions,
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=0.1
    )
    
    print("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Q-learning...")
    
    # ‡∏ù‡∏∂‡∏Å agent
    agent.train(env, n_episodes=1000, verbose=True)
    
    print("\n‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö agent
    print("\n‡∏ó‡∏î‡∏™‡∏≠‡∏ö trained agent:")
    total_reward, steps, path = agent.test_episode(env)
    print(f"Total reward: {total_reward:.2f}")
    print(f"Steps taken: {steps}")
    print(f"Path: {path}")
    
    # ‡πÅ‡∏™‡∏î‡∏á Q-table
    print("\nQ-table (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):")
    print("State | Up    | Down  | Left  | Right")
    print("-" * 40)
    for state in range(min(10, env.n_states)):
        q_vals = agent.q_table[state]
        print(f"{state:5d} | {q_vals[0]:5.2f} | {q_vals[1]:5.2f} | {q_vals[2]:5.2f} | {q_vals[3]:5.2f}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á visualizations
    create_visualizations(env, agent)

def create_visualizations(env, agent):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ"""
    print("\n‡∏™‡∏£‡πâ‡∏≤‡∏á visualizations...")
    
    # 1. Learning curve
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    window_size = 50
    if len(agent.episode_rewards) >= window_size:
        smoothed_rewards = np.convolve(agent.episode_rewards, 
                                     np.ones(window_size)/window_size, mode='valid')
        plt.plot(smoothed_rewards)
    else:
        plt.plot(agent.episode_rewards)
    plt.title('Learning Curve (Smoothed Rewards)')
    plt.xlabel('Episode')
    plt.ylabel('Average Reward')
    plt.grid(True)
    
    # 2. Grid world with Q-values
    plt.subplot(1, 3, 2)
    plt.title('Grid World Environment')
    env.visualize()
    
    # 3. Optimal Policy
    plt.subplot(1, 3, 3)
    plt.title('Learned Policy')
    policy = agent.get_policy()
    env.visualize(policy=policy)
    
    plt.tight_layout()
    plt.savefig('q_learning_results.png', dpi=150, bbox_inches='tight')
    print("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô 'q_learning_results.png'")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
    plt.show()

def interactive_demo():
    """Demo ‡πÅ‡∏ö‡∏ö interactive"""
    if not HAS_PACKAGES:
        print("Interactive demo ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ numpy ‡πÅ‡∏•‡∏∞ matplotlib")
        return
    
    print("=== Interactive Q-Learning Demo ===")
    print("‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏î‡πâ")
    print()
    
    # ‡∏£‡∏±‡∏ö input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    try:
        grid_size = int(input("‡∏Ç‡∏ô‡∏≤‡∏î grid (3-8, default=5): ") or "5")
        learning_rate = float(input("Learning rate (0.01-1.0, default=0.1): ") or "0.1")
        epsilon = float(input("Epsilon (0.01-1.0, default=0.1): ") or "0.1")
        episodes = int(input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô episodes (100-5000, default=1000): ") or "1000")
    except ValueError:
        print("‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default")
        grid_size, learning_rate, epsilon, episodes = 5, 0.1, 0.1, 1000
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô experiment
    env = GridWorld(size=grid_size)
    agent = QLearningAgent(
        n_states=env.n_states,
        n_actions=env.n_actions,
        learning_rate=learning_rate,
        discount_factor=0.95,
        epsilon=epsilon
    )
    
    print(f"\n‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ù‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:")
    print(f"Grid size: {grid_size}x{grid_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Epsilon: {epsilon}")
    print(f"Episodes: {episodes}")
    print()
    
    agent.train(env, n_episodes=episodes, verbose=True)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    total_reward, steps, path = agent.test_episode(env)
    print(f"\n‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    print(f"Total reward: {total_reward:.2f}")
    print(f"Steps taken: {steps}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û
    create_visualizations(env, agent)

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("ü§ñ Q-Learning Demo for AI Course")
    print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Q-learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô AI")
    print("=" * 50)
    print()
    
    if HAS_PACKAGES:
        print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï:")
        print("1. Basic Demo (‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô)")
        print("2. Full Demo (‡∏û‡∏£‡πâ‡∏≠‡∏° visualizations)")
        print("3. Interactive Demo (‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏î‡πâ)")
        print("4. ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°")
        print()
        
        while True:
            try:
                choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1-4): ").strip()
                if choice == '1':
                    run_basic_demo()
                    break
                elif choice == '2':
                    run_full_demo()
                    break
                elif choice == '3':
                    interactive_demo()
                    break
                elif choice == '4':
                    print("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
                    break
                else:
                    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1-4")
            except KeyboardInterrupt:
                print("\n‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!")
                break
    else:
        print("‡∏£‡∏±‡∏ô‡πÇ‡∏´‡∏°‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ external packages)")
        run_basic_demo()

if __name__ == "__main__":
    main()
