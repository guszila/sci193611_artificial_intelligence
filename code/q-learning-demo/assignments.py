#!/usr/bin/env python3
"""
Q-Learning Assignment - Grid World Challenge
‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î Q-learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô

‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ assignment ‡∏ï‡πà‡∏≤‡∏á‡πÜ
"""

from simple_q_learning import SimpleGridWorld, SimpleQLearning

def assignment_1_basic():
    """
    Assignment 1: Basic Q-Learning
    ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô Q-learning ‡πÉ‡∏ô Grid World 4x4 ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
    """
    print("=== Assignment 1: Basic Q-Learning ===")
    print()
    
    # TODO: ‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÅ‡∏•‡∏∞ agent
    env = SimpleGridWorld(size=4)
    agent = SimpleQLearning(
        n_states=16,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.1
    )
    
    print("Grid World Setup:")
    env.print_grid()
    
    # TODO: ‡∏ù‡∏∂‡∏Å agent
    print("Training...")
    agent.train(env, episodes=500)
    
    # TODO: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•
    print("\nTesting trained agent:")
    reward, steps, path = agent.test(env, show_path=False)
    print(f"Total reward: {reward}")
    print(f"Steps taken: {steps}")
    
    # ‡πÅ‡∏™‡∏î‡∏á Q-table ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
    print("\nQ-Table (first 8 states):")
    print("State |   ‚Üë   |   ‚Üì   |   ‚Üê   |   ‚Üí   ")
    print("-" * 40)
    for state in range(8):
        q_vals = agent.q_table[state]
        print(f"{state:5d} | {q_vals[0]:5.2f} | {q_vals[1]:5.2f} | {q_vals[2]:5.2f} | {q_vals[3]:5.2f}")
    
    print("\n--- Questions for Assignment 1 ---")
    print("1. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏≥‡πÑ‡∏° Q-value ‡∏Ç‡∏≠‡∏á state ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ goal ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤")
    print("2. ‡∏ó‡∏≥‡πÑ‡∏° epsilon-greedy policy ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ")
    print("3. ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô learning rate ‡πÄ‡∏õ‡πá‡∏ô 0.01 ‡πÅ‡∏•‡∏∞ 0.5 ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•")

def assignment_2_parameter_study():
    """
    Assignment 2: Parameter Study
    ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    """
    print("=== Assignment 2: Parameter Study ===")
    print()
    
    # ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö learning rates ‡∏ï‡πà‡∏≤‡∏á‡πÜ
    learning_rates = [0.01, 0.1, 0.3, 0.7]
    print("Testing different learning rates:")
    
    for lr in learning_rates:
        env = SimpleGridWorld(size=4)
        agent = SimpleQLearning(
            n_states=16,
            n_actions=4,
            learning_rate=lr,
            discount=0.9,
            epsilon=0.1
        )
        
        agent.train(env, episodes=500)
        reward, steps, _ = agent.test(env, show_path=False)
        
        print(f"Learning Rate {lr}: Final reward = {reward:.2f}, Steps = {steps}")
    
    print("\n--- Questions for Assignment 2 ---")
    print("1. Learning rate ‡πÑ‡∏´‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î? ‡∏ó‡∏≥‡πÑ‡∏°?")
    print("2. ‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö epsilon values: 0.01, 0.1, 0.3, 0.7")
    print("3. ‡∏•‡∏≠‡∏á‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö discount factor: 0.5, 0.7, 0.9, 0.99")

def assignment_3_environment_design():
    """
    Assignment 3: Environment Design
    ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö environment ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    """
    print("=== Assignment 3: Environment Design ===")
    print()
    
    # TODO: ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á environment ‡πÉ‡∏´‡∏°‡πà
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Grid World ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
    
    class CustomGridWorld(SimpleGridWorld):
        def __init__(self):
            super().__init__(size=5)
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡πÉ‡∏´‡∏°‡πà
            self.obstacles = [(1, 1), (1, 2), (2, 1), (3, 3)]
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô reward structure
            self.goal_reward = 20
            self.obstacle_penalty = -10
    
    env = CustomGridWorld()
    print("Custom Grid World:")
    env.print_grid()
    
    agent = SimpleQLearning(
        n_states=25,
        n_actions=4,
        learning_rate=0.1,
        discount=0.9,
        epsilon=0.2
    )
    
    agent.train(env, episodes=1000)
    reward, steps, _ = agent.test(env, show_path=False)
    print(f"Custom environment result: Reward = {reward:.2f}, Steps = {steps}")
    
    print("\n--- Tasks for Assignment 3 ---")
    print("1. ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Grid World ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á (‡∏Ç‡∏ô‡∏≤‡∏î, ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ, rewards)")
    print("2. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏±‡∏ö standard environment")
    print("3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤ environment design ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ learning ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£")

def assignment_4_advanced():
    """
    Assignment 4: Advanced Modifications
    ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á algorithm ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà
    """
    print("=== Assignment 4: Advanced Modifications ===")
    print()
    
    # TODO: ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à
    
    print("Choose one of the following topics:")
    print("1. Implement SARSA algorithm ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Q-Learning")
    print("2. Add epsilon decay strategy ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤")
    print("3. Implement Double Q-Learning")
    print("4. Add experience replay")
    print("5. Create multi-goal environment")
    print("6. Implement priority sweeping")
    
    print("\nExample: Simple SARSA Implementation")
    
    class SARSAAgent(SimpleQLearning):
        """SARSA Agent - On-policy learning"""
        
        def train_episode(self, env, max_steps=100):
            state = env.reset()
            action = self.get_action(state)  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å action ‡πÅ‡∏£‡∏Å
            total_reward = 0
            
            for _ in range(max_steps):
                next_state, reward, done = env.step(action)
                next_action = self.get_action(next_state) if not done else 0
                
                # SARSA update: ‡πÉ‡∏ä‡πâ actual next action ‡πÅ‡∏ó‡∏ô max
                target = reward + self.gamma * self.q_table[next_state][next_action]
                error = target - self.q_table[state][action]
                self.q_table[state][action] += self.lr * error
                
                total_reward += reward
                state, action = next_state, next_action
                
                if done:
                    break
            
            return total_reward, 0
    
    print("SARSA vs Q-Learning comparison example implemented above.")

def bonus_visualization():
    """
    Bonus: Enhanced Visualization
    ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à)
    """
    print("=== Bonus: Enhanced Visualization ===")
    print()
    
    # TODO: ‡πÉ‡∏ä‡πâ matplotlib ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü learning curve
    # TODO: ‡∏™‡∏£‡πâ‡∏≤‡∏á animation ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
    # TODO: ‡πÅ‡∏™‡∏î‡∏á heatmap ‡∏Ç‡∏≠‡∏á Q-values
    
    print("Ideas for enhanced visualization:")
    print("1. Plot learning curves with matplotlib")
    print("2. Create heatmap of Q-values")
    print("3. Animate the learning process")
    print("4. Show value function as 3D surface")
    print("5. Create policy visualization with arrows")
    
    print("\nSample code for learning curve:")
    print("""
import matplotlib.pyplot as plt

def plot_learning_curve(episode_rewards):
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards)
    plt.title('Q-Learning Performance')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.grid(True)
    plt.show()
    """)

def main():
    """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å assignment ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥"""
    print("üéì Q-Learning Assignments")
    print("=========================")
    print()
    
    assignments = {
        '1': assignment_1_basic,
        '2': assignment_2_parameter_study,
        '3': assignment_3_environment_design,
        '4': assignment_4_advanced,
        '5': bonus_visualization
    }
    
    while True:
        print("Available assignments:")
        print("1. Basic Q-Learning (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)")
        print("2. Parameter Study (‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå)")
        print("3. Environment Design (‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö environment)")
        print("4. Advanced Modifications (‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á)")
        print("5. Bonus: Visualization (‡πÄ‡∏™‡∏£‡∏¥‡∏°)")
        print("6. Exit")
        print()
        
        choice = input("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å assignment (1-6): ").strip()
        
        if choice in assignments:
            print()
            assignments[choice]()
            print("\n" + "="*50 + "\n")
        elif choice == '6':
            print("Good luck with your assignments! üöÄ")
            break
        else:
            print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1-6")

if __name__ == "__main__":
    main()
