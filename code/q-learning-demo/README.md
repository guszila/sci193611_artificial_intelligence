# Q-Learning Demo Project

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ Q-learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô Reinforcement Learning
**üÜï ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö!**

## ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

### 1. `simple_q_learning.py` (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï) ‚≠ê
- ‡πÉ‡∏ä‡πâ Python ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á package ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
- ‡πÄ‡∏´‡∏°## ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏ï‡πà‡∏≠

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á:
- ‡πÄ‡∏û‡∏¥‡πà‡∏° different RL algorithms (SARSA, Double Q-Learning)
- ‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö environments ‡∏≠‡∏∑‡πà‡∏ô (Frozen Lake, Cart Pole)
- **‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° neural network function approximation (‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!)**
- ‡πÄ‡∏û‡∏¥‡πà‡∏° advanced DQN features (Double DQN, Dueling DQN)
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö other planning algorithms
- **‡∏™‡∏£‡πâ‡∏≤‡∏á visualization ‡∏Ç‡∏≠‡∏á neural network learning**

### ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ï‡πà‡∏≠:
1. **Convolutional Neural Network** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö visual input
2. **Policy Gradient Methods** (REINFORCE, Actor-Critic)
3. **Multi-agent Reinforcement Learning**
4. **Real-world applications** (robotics, games)‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
- **üÜï ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 4 ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å: Easy, Normal, Hard, Maze**
- ‡∏°‡∏µ interactive mode ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÑ‡∏î‡πâ
- ‡∏°‡∏µ‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà

### 2. `grid_world.py`
- Grid World environment ‡∏û‡∏£‡πâ‡∏≠‡∏° visualization
- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ numpy ‡πÅ‡∏•‡∏∞ matplotlib

### 3. `q_learning.py`
- Q-Learning algorithm ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö epsilon decay ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### 4. `demo.py`
- Demo script ‡∏£‡∏ß‡∏° ‡∏°‡∏µ 3 ‡πÇ‡∏´‡∏°‡∏î
- ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á numpy ‡πÅ‡∏•‡∏∞ matplotlib ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö full version

### 5. `assignments.py`
- ‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
- ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å

### 7. `pure_python_dqn.py` & `test_dqn.py` (‡πÉ‡∏´‡∏°‡πà!) üß†
- **Pure Python DQN**: Neural Network Function Approximation
- ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á numpy ‡∏´‡∏£‡∏∑‡∏≠ pytorch
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular Q-Learning ‡∏Å‡∏±‡∏ö Deep Q-Network
- ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á neural network ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢

### 8. `educational_comparison.py` (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô!) üéì
- ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î step-by-step
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏î‡∏µ/‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á
- ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö learning curves
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô

### 8. `test_dqn.py` (‡∏ó‡∏î‡∏™‡∏≠‡∏ö Neural Network)
- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Pure Python neural network implementation
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Tabular vs DQN

## ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà üéØ

### 1. **Easy** (‡∏á‡πà‡∏≤‡∏¢)
- ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ô‡πâ‡∏≠‡∏¢ ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- Grid 4x4: 2 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ
- Grid 5x5: 3 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ

### 2. **Normal** (‡∏õ‡∏Å‡∏ï‡∏¥)
- ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏°‡∏µ‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏´‡∏•‡∏±‡∏Å
- Grid 4x4: 2 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ (‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á)
- Grid 5x5: 4 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ

### 3. **Hard** (‡∏¢‡∏≤‡∏Å)
- ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏°‡∏≤‡∏Å ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏ó‡∏≤‡∏á
- Grid 4x4: 4 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ
- Grid 5x5: 6 ‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ

### 4. **Maze** (‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï) üåÄ
- ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Ç‡∏≤‡∏ß‡∏á‡∏Å‡∏ï ‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏ä‡πà‡∏≠‡∏á‡πÅ‡∏Ñ‡∏ö
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏¥‡∏î pattern

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) ‚≠ê
```bash
cd q-learning-demo
python3 simple_q_learning.py
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 = Demo ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å 
```bash
python3 simple_q_learning.py
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 3 = ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: Interactive Mode
```bash
python3 simple_q_learning.py
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 2 = ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ó‡∏∏‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 5: Neural Network Function Approximation 
```bash
python3 pure_python_dqn.py
# ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular Q-Learning ‡∏Å‡∏±‡∏ö Deep Q-Network
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 6: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Neural Network ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
```bash
python3 test_dqn.py  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö neural network implementation
```

## ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≤‡∏á‡πÜ

### Easy Level
```
Grid World 4x4 - easy
‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: 2, ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: 6 steps
Final reward: 9.20, Steps: 7
‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: 85.7%
```

### Hard Level  
```
Grid World 4x4 - hard
‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ: 4, ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: 6 steps
Final reward: 8.60, Steps: 12
‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û: 50.0%
```

### Deep Q-Network (DQN) Example
```
Pure Python DQN vs Tabular Q-Learning:
Grid World 3x3 - easy
Neural Network: Input=9, Hidden=32, Output=4

Tabular Q-Learning: 9.70 reward, 4 steps
Pure Python DQN:   -5.00 reward, 50 steps (needs more training)

‚Üí ‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡πá‡∏Å Tabular ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤
‚Üí DQN ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÄ‡∏°‡∏∑‡πà‡∏≠ state space ‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å
```

## üéì ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô

### ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô:

1. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Simple Demo** (15 ‡∏ô‡∏≤‡∏ó‡∏µ)
   ```bash
   python simple_q_learning.py
   ```
   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å interactive mode
   - ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô Q-table ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå: state, action, reward, Q-value

2. **‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô** (10 ‡∏ô‡∏≤‡∏ó‡∏µ)
   ```bash
   python simple_q_learning.py
   ```
   - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å difficulty levels: easy ‚Üí normal ‚Üí hard ‚Üí maze
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤ state space ‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠ Q-table ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

3. **Neural Network vs Tabular** (20 ‡∏ô‡∏≤‡∏ó‡∏µ)
   ```bash
   python educational_comparison.py
   ```
   - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á tabular approach
   - ‡πÅ‡∏™‡∏î‡∏á function approximation concept
   - ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô

### ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥:

1. **‡πÅ‡∏ö‡∏ö‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**
   ```bash
   python assignments.py
   ```

2. **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á parameters**
   - ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç learning_rate, epsilon, discount
   - ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à:

- "‡∏ñ‡πâ‡∏≤ state space ‡∏°‡∏µ 1 ‡∏•‡πâ‡∏≤‡∏ô states ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô?"
- "‡∏ó‡∏≥‡πÑ‡∏° neural network ‡∏ñ‡∏∂‡∏á‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤‡πÉ‡∏ô grid world ‡πÄ‡∏•‡πá‡∏Å?"
- "‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ function approximation?"

## üéØ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ

### 1. **‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á**
- ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Success Rate)
- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏ï‡πà‡∏≠‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà

### 2. **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥**
- Episodes ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏¢‡∏≤‡∏Å
- Epsilon ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö environment ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô

### 4. **Neural Network Function Approximation** üß†
- Pure Python implementation ‡∏Ç‡∏≠‡∏á Deep Q-Network
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Tabular Q-Learning
- ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏î‡∏µ/‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á
- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á tabular ‡∏Å‡∏±‡∏ö function approximation

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢:
1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Easy level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î
2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Hard level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢
3. ‡πÉ‡∏ä‡πâ Maze level ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î
4. **‡πÅ‡∏™‡∏î‡∏á neural network function approximation**

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hands-on:
1. ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö
2. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á
4. **‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Tabular vs DQN**

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢:
1. ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á
2. ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö
3. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
4. **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö tabular ‡∏Å‡∏±‡∏ö function approximation**

## ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏∞‡∏î‡∏±‡∏ö

| ‡∏£‡∏∞‡∏î‡∏±‡∏ö | Episodes | Epsilon | Learning Rate | ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏ |
|-------|----------|---------|---------------|----------|
| Easy | 800 | 0.2 | 0.1 | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß |
| Normal | 1000 | 0.3 | 0.1 | ‡∏Ñ‡πà‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥ |
| Hard | 1500 | 0.4 | 0.1 | ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ exploration ‡∏°‡∏≤‡∏Å |
| Maze | 2000 | 0.5 | 0.05 | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô |

## Neural Network Function Approximation üß†

### Pure Python Implementation:
- **SimpleMLP**: Multi-layer perceptron ‡πÉ‡∏ä‡πâ pure Python
- **PurePythonDQN**: Deep Q-Network implementation
- **Target Network**: Stable learning ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô DQN paper
- **Experience Replay**: ‡πÄ‡∏Å‡πá‡∏ö transitions ‡πÅ‡∏•‡∏∞ sample ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°

### ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Tabular ‡∏Å‡∏±‡∏ö Function Approximation:

| ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ | ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢ | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö |
|--------|-------|---------|----------|
| **Tabular Q-Learning** | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß, ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô | ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ state space | ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏•‡πá‡∏Å |
| **Neural Network DQN** | ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö state ‡πÉ‡∏´‡∏ç‡πà, generalize ‡πÑ‡∏î‡πâ | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤, ‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ | ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏´‡∏ç‡πà |

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
```python
# Tabular Q-Learning
tabular_agent = SimpleQLearning(n_states=16, n_actions=4)

# Neural Network DQN  
dqn_agent = PurePythonDQN(state_size=16, action_size=4)
```

### ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÉ‡∏ô‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô:
1. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Tabular**: ‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÑ‡∏î‡πâ
2. **‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î**: ‡πÄ‡∏°‡∏∑‡πà‡∏≠ state space ‡πÉ‡∏´‡∏ç‡πà
3. **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Neural Network**: ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤
4. **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**: ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏î‡∏µ/‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢

## ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å

1. **‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à**: ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
2. **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÑ‡∏î‡πâ**: ‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á algorithm
3. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à**: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡πâ‡∏≤‡∏ó‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô
4. **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á**: ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÇ‡∏•‡∏Å‡∏à‡∏£‡∏¥‡∏á

---

*‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô Q-learning ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ï‡πà‡∏≤‡∏á‡πÜ*

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
```bash
cd q-learning-demo
python simple_q_learning.py
```

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: Full version (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages)
```bash
pip install numpy matplotlib
python demo.py
```

## ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å

### 1. Grid World Environment
- ‡∏Ç‡∏ô‡∏≤‡∏î‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ (3x3 ‡∏ñ‡∏∂‡∏á 8x8)
- ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (S), ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (G), ‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ (X)
- Reward system: Goal=+10, Obstacle=-5, Step=-0.1

### 2. Q-Learning Algorithm
- Epsilon-greedy exploration
- Temporal difference learning
- Q-value updates ‡∏ï‡∏≤‡∏° Bellman equation

### 3. Visualizations
- Grid world map
- Learning curves
- Optimal policy
- Q-table values

### 4. Interactive Features
- ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á learning rate, epsilon, episodes
- ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô step-by-step
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å

## ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

- **Learning Rate (Œ±)**: 0.1 (‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)
- **Discount Factor (Œ≥)**: 0.9-0.95 (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á future rewards)
- **Epsilon (Œµ)**: 0.1-0.2 (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à)
- **Episodes**: 1000+ (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å)

## ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢:
1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ `simple_q_learning.py` ‡πÇ‡∏´‡∏°‡∏î demo
2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ concepts ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏î‡∏π‡∏ú‡∏• Q-table
3. ‡πÅ‡∏™‡∏î‡∏á learning process ‡πÅ‡∏ö‡∏ö step-by-step

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hands-on:
1. ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏±‡∏ô interactive mode
2. ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ú‡∏•
3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏ï‡πà‡∏≤‡∏á‡πÜ

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏ö‡∏´‡∏°‡∏≤‡∏¢:
1. ‡πÉ‡∏´‡πâ‡∏î‡∏±‡∏î‡πÅ‡∏õ‡∏•‡∏á reward structure
2. ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏≠‡∏∏‡∏õ‡∏™‡∏£‡∏£‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô grid size
3. ‡∏ó‡∏î‡∏•‡∏≠‡∏á different exploration strategies

## ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

```
Episode 0: Average reward = -2.50, Epsilon = 0.200
Episode 200: Average reward = 5.20, Epsilon = 0.190
Episode 400: Average reward = 7.80, Epsilon = 0.181
Episode 600: Average reward = 8.90, Epsilon = 0.172
Episode 800: Average reward = 9.10, Epsilon = 0.164

Final performance: Reward=9.90, Steps=8
```

## Q-Table ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

```
State |   ‚Üë   |   ‚Üì   |   ‚Üê   |   ‚Üí   
----------------------------------------
    0 | -0.50 |  1.20 | -0.30 |  2.10
    1 |  0.80 |  2.50 | -0.10 |  3.40
    2 |  1.90 |  4.20 |  0.60 |  5.80
   ...
```



## ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ú‡∏•‡∏ï‡πà‡∏≠

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏¢‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á:
- ‡πÄ‡∏û‡∏¥‡πà‡∏° different RL algorithms (SARSA, Double Q-Learning)
- ‡∏•‡∏≠‡∏á‡∏Å‡∏±‡∏ö environments ‡∏≠‡∏∑‡πà‡∏ô (Frozen Lake, Cart Pole)
- ‡πÄ‡∏û‡∏¥‡πà‡∏° neural network function approximation
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö other planning algorithms

---

*‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô Q-learning ‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏£‡∏¥‡∏ç‡∏ç‡∏≤‡∏ï‡∏£‡∏µ *
