{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Making Decisions - Markov Decision Processes (MDPs)\n",
    "\n",
    "## Overview\n",
    "บทบรรยายนี้แสดงการประยุกต์ใช้ทฤษฎี **Markov Decision Process (MDP)** ในการแก้ปัญหาการตัดสินใจภายใต้ความไม่แน่นอน\n",
    "\n",
    "### หัวข้อหลัก:\n",
    "1. **Grid World Environment** - สภาพแวดล้อมจำลองที่มีความไม่แน่นอน\n",
    "2. **Value Iteration** - อัลกอริทึมสำหรับหาค่าที่เหมาะสมของสถานะ\n",
    "3. **Policy Iteration** - อัลกอริทึมสำหรับหา policy ที่เหมาะสมโดยตรง\n",
    "4. **Bellman Equation** - สมการพื้นฐานสำหรับคำนวณค่าของสถานะ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, height=3, width=4):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.grid = np.zeros((height, width))\n",
    "        self.terminal_states = {(0, 3): 1, (1, 3): -1}  # (state): reward\n",
    "        self.living_reward = -0.04\n",
    "        self.gamma = 1.0\n",
    "        self.p_intended = 0.8\n",
    "        self.p_perpendicular = 0.1  # For each perpendicular direction\n",
    "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
    "        \n",
    "    def is_valid_state(self, state):\n",
    "        row, col = state\n",
    "        if row < 0 or row >= self.height or col < 0 or col >= self.width:\n",
    "            return False\n",
    "        if (row, col) == (1, 1):  # Wall\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def get_transition_probs(self, state, action):\n",
    "        if state in self.terminal_states:\n",
    "            return [(state, 1.0)]\n",
    "        \n",
    "        transitions = []\n",
    "\n",
    "        perp1 = (action[1], action[0])    # Rotate 90° clockwise\n",
    "        perp2 = (-action[1], -action[0])  # Rotate 90° counterclockwise\n",
    "        \n",
    "        for next_action, prob in [(action, self.p_intended), \n",
    "                                  (perp1, self.p_perpendicular),\n",
    "                                  (perp2, self.p_perpendicular)]:\n",
    "            next_state = (state[0] + next_action[0], state[1] + next_action[1])\n",
    "            if self.is_valid_state(next_state):\n",
    "                transitions.append((next_state, prob))\n",
    "            else:\n",
    "                transitions.append((state, prob))  # Stay in current state\n",
    "                \n",
    "        return transitions\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        if state in self.terminal_states:\n",
    "            return self.terminal_states[state]\n",
    "        return self.living_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GridWorld Class - การจำลอง Markov Decision Process (MDP)\n",
    "\n",
    "คลาส `GridWorld` นี้จำลองสภาพแวดล้อมตาม **Grid World** ที่อธิบายในบทบรรยาย โดยมีองค์ประกอบสำคัญของ MDP:\n",
    "\n",
    "### องค์ประกอบของ MDP: $(\\mathcal{S}, \\mathcal{A}, P, R)$\n",
    "- **$\\mathcal{S}$ (States)**: ตำแหน่งต่างๆ ในตาราง 3×4\n",
    "- **$\\mathcal{A}$ (Actions)**: การเคลื่อนที่ 4 ทิศทาง [Right, Down, Left, Up]\n",
    "- **$P$ (Transition Model)**: ความน่าจะเป็นการเปลี่ยนสถานะ (มีความไม่แน่นอน)\n",
    "- **$R$ (Reward Function)**: รางวัลที่ได้รับในแต่ละสถานะ\n",
    "\n",
    "### ความไม่แน่นอนในการเคลื่อนที่ (Stochastic Actions):\n",
    "- **80%** ไปในทิศทางที่ตั้งใจ (`p_intended = 0.8`)\n",
    "- **10%** ไปในทิศทางตั้งฉากด้านหนึ่ง (`p_perpendicular = 0.1`)\n",
    "- **10%** ไปในทิศทางตั้งฉากอีกด้านหนึ่ง\n",
    "\n",
    "### ค่ารางวัล:\n",
    "- **Living reward**: -0.04 (รางวัลเล็กน้อยในแต่ละขั้นตอน)\n",
    "- **Terminal states**: +1 ที่ตำแหน่ง (0,3), -1 ที่ตำแหน่ง (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value Iteration Algorithm\n",
    "\n",
    "**Value Iteration** เป็นอัลกอริทึมสำหรับหาค่าที่เหมาะสมของแต่ละสถานะ $V(s)$ โดยใช้ **Bellman Update**:\n",
    "\n",
    "$$V_{i+1}(s) := R(s) + \\gamma \\max_a \\sum_{s'} P(s'|s,a) V_i(s')$$\n",
    "\n",
    "### วิธีการทำงาน:\n",
    "1. **เริ่มต้น**: กำหนดค่า $V_0(s) = 0$ สำหรับทุกสถานะ\n",
    "2. **อัพเดท**: คำนวณค่า $V$ ใหม่สำหรับทุกสถานะพร้อมกัน\n",
    "3. **ทำซ้ำ**: จนกว่าจะลู่เข้า (convergence)\n",
    "\n",
    "### ขั้นตอนในโค้ด:\n",
    "- คำนวณ **Q-value** สำหรับแต่ละ action: $Q(s,a) = \\sum_{s'} P(s'|s,a) V(s')$\n",
    "- หาค่าสูงสุด: $\\max_a Q(s,a)$\n",
    "- อัพเดทค่า: $V_{new}(s) = R(s) + \\gamma \\max_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, threshold=1e-3):\n",
    "    # Initialize values\n",
    "    V = {(i, j): 0 for i in range(grid.height) for j in range(grid.width) \n",
    "            if grid.is_valid_state((i, j))}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        # Update each state\n",
    "        for state in V:\n",
    "            if state in grid.terminal_states:\n",
    "                V_new[state] = grid.get_reward(state)\n",
    "                \n",
    "            else:\n",
    "                # Calculate max_a \\sum_{s'} P(s'|s,a) V(s')\n",
    "                max_q = float('-inf')\n",
    "\n",
    "                for action in grid.actions:\n",
    "                    q = 0\n",
    "                    for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                        q += prob * V[next_state]\n",
    "                    max_q = max(max_q, q)\n",
    "                \n",
    "                V_new[state] = grid.get_reward(state) + grid.gamma * max_q\n",
    "            biggest_change = max(biggest_change, abs(V_new[state] - V[state]))\n",
    "        \n",
    "        V = V_new\n",
    "        iteration += 1\n",
    "        \n",
    "        # Check convergence\n",
    "        if biggest_change < threshold:\n",
    "            break\n",
    "            \n",
    "    return V, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged after 20 iterations\n",
      "\n",
      "Final values:\n",
      "   0.812    0.868    0.918    1.000 \n",
      "   0.762    XXXXX    0.660   -1.000 \n",
      "   0.705    0.655    0.611    0.387 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid = GridWorld()\n",
    "V, iterations = value_iteration(grid)\n",
    "\n",
    "print(f\"\\nConverged after {iterations} iterations\")\n",
    "print(\"\\nFinal values:\")\n",
    "for i in range(grid.height):\n",
    "    for j in range(grid.width):\n",
    "        if not grid.is_valid_state((i, j)):\n",
    "            print(\"   XXXXX \", end=\"\")\n",
    "        else:\n",
    "            print(f\" {V[(i, j)]:7.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): (0, 1),\n",
       " (0, 1): (0, 1),\n",
       " (0, 2): (0, 1),\n",
       " (0, 3): None,\n",
       " (1, 0): (-1, 0),\n",
       " (1, 2): (-1, 0),\n",
       " (1, 3): None,\n",
       " (2, 0): (-1, 0),\n",
       " (2, 1): (0, -1),\n",
       " (2, 2): (0, -1),\n",
       " (2, 3): (0, -1)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_extraction(grid, V):\n",
    "    policy = {state: None for state in V}\n",
    "    \n",
    "    for state in V:\n",
    "        if state in grid.terminal_states:\n",
    "            policy[state] = None\n",
    "        \n",
    "        else:\n",
    "            max_q = float('-inf')\n",
    "            best_action = None\n",
    "            for action in grid.actions:\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                if q > max_q:\n",
    "                    max_q = q\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "        \n",
    "    return policy\n",
    "\n",
    "policy = policy_extraction(grid, V)\n",
    "policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Extraction - การสกัด Optimal Policy\n",
    "\n",
    "หลังจากได้ค่า $V(s)$ ที่เหมาะสมแล้ว เราสามารถสกัด **Optimal Policy** ได้โดยใช้หลักการ **Maximum Expected Utility**:\n",
    "\n",
    "$$\\pi^*(s) = \\arg \\max_a \\sum_{s'} P(s'|s,a) V(s')$$\n",
    "\n",
    "### ความหมาย:\n",
    "- สำหรับแต่ละสถานะ $s$ ให้เลือก action $a$ ที่ทำให้ได้ค่า expected utility สูงสุด\n",
    "- นี่คือการใช้ **one-step lookahead** เพื่อเลือก action ที่ดีที่สุด\n",
    "\n",
    "### ในโค้ด:\n",
    "- คำนวณ Q-value สำหรับทุก action\n",
    "- เลือก action ที่ให้ค่า Q สูงสุด\n",
    "- นั่นคือ optimal action สำหรับสถานะนั้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t→\t→\t→\t*\n",
      "\t↑\tX\t↑\t*\n",
      "\t↑\t←\t←\t←\n"
     ]
    }
   ],
   "source": [
    "def print_policy(grid, policy):\n",
    "    for i in range(grid.height):\n",
    "        for j in range(grid.width):\n",
    "            if not grid.is_valid_state((i, j)):\n",
    "                print(\"\\tX\", end=\"\")\n",
    "            else:\n",
    "                if policy[(i, j)] == (0, 1):\n",
    "                    print(\"\\t\\u2192\", end=\"\")\n",
    "                elif policy[(i, j)] == (1, 0):\n",
    "                    print(\"\\t\\u2193\", end=\"\")\n",
    "                elif policy[(i, j)] == (0, -1):\n",
    "                    print(\"\\t\\u2190\", end=\"\")\n",
    "                elif policy[(i, j)] == (-1, 0):\n",
    "                    print(\"\\t\\u2191\", end=\"\")\n",
    "                else:\n",
    "                    print(\"\\t*\", end=\"\")\n",
    "        print()\n",
    "\n",
    "print_policy(grid, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Policy Visualization\n",
    "\n",
    "ฟังก์ชัน `print_policy` แสดง optimal policy ในรูปแบบที่เข้าใจง่าย:\n",
    "- **→** (Right): เคลื่อนที่ไปทางขวา\n",
    "- **↓** (Down): เคลื่อนที่ไปทางล่าง  \n",
    "- **←** (Left): เคลื่อนที่ไปทางซ้าย\n",
    "- **↑** (Up): เคลื่อนที่ไปทางบน\n",
    "- **X**: กำแพง/สิ่งกีดขวาง\n",
    "- **\\***: สถานะสิ้นสุด (terminal states)\n",
    "\n",
    "### ความสำคัญของ Living Reward:\n",
    "ค่า living reward (-0.04) มีผลต่อ optimal policy:\n",
    "- **ค่าลบ**: agent จะพยายามไปถึงเป้าหมายเร็วที่สุด\n",
    "- **ค่าบวก**: agent อาจจะเดินอ้อมเพื่อได้รางวัลมากขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Policy Iteration Algorithm\n",
    "\n",
    "**Policy Iteration** เป็นอีกวิธีหนึ่งในการหา optimal policy โดยตรง แทนที่จะหาค่า V ก่อน\n",
    "\n",
    "### ข้อดีของ Policy Iteration:\n",
    "- มักจะ **converge เร็วกว่า** Value Iteration\n",
    "- หา policy โดยตรงแทนที่จะต้องสกัดจาก V\n",
    "- เหมาะสำหรับปัญหาที่มี action space ใหญ่\n",
    "\n",
    "### อัลกอริทึมทำงาน 2 ขั้นตอนสลับกัน:\n",
    "\n",
    "#### 1. Policy Evaluation\n",
    "คำนวณ $V^{\\pi}(s)$ สำหรับ policy ปัจจุบัน โดยแก้สมการ Bellman แบบเชิงเส้น:\n",
    "$$V^{\\pi}(s) = R(s) + \\gamma \\sum_{s'} P(s'|s,\\pi(s)) V^{\\pi}(s')$$\n",
    "\n",
    "#### 2. Policy Improvement  \n",
    "ปรับปรุง policy ให้ดีขึ้นโดยใช้ one-step lookahead:\n",
    "$$\\pi_{new}(s) = \\arg \\max_a \\sum_{s'} P(s'|s,a) V^{\\pi}(s')$$\n",
    "\n",
    "### หยุดทำงานเมื่อ:\n",
    "Policy ไม่เปลี่ยนแปลงอีก (policy stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(grid, policy, updates=20):\n",
    "    V = {state: 0 for state in policy}\n",
    "\n",
    "    for _ in range(updates):\n",
    "        V_new = V.copy()\n",
    "        for state in V:\n",
    "            if state in grid.terminal_states:\n",
    "                V_new[state] = grid.get_reward(state)\n",
    "\n",
    "            else:\n",
    "                action = policy[state]\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                V_new[state] = grid.get_reward(state) + grid.gamma * q\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_iteration(grid, threshold=1e-3):\n",
    "    # Initialize policy\n",
    "    policy = {(i, j): grid.actions[0] for i in range(grid.height) for j in range(grid.width) \n",
    "            if grid.is_valid_state((i, j))}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        # Policy evaluation\n",
    "        V = policy_evaluation(grid, policy)\n",
    "        \n",
    "        # Policy improvement\n",
    "        policy_stable = True\n",
    "        for state in V:\n",
    "            old_action = policy[state]\n",
    "            max_q = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            for action in grid.actions:\n",
    "                q = 0\n",
    "                for next_state, prob in grid.get_transition_probs(state, action):\n",
    "                    q += prob * V[next_state]\n",
    "                if q > max_q:\n",
    "                    max_q = q\n",
    "                    best_action = action\n",
    "                    \n",
    "            policy[state] = best_action\n",
    "            if best_action != old_action:\n",
    "                policy_stable = False\n",
    "        \n",
    "        iteration += 1\n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    return policy, V, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation Function\n",
    "\n",
    "ฟังก์ชัน `policy_evaluation` คำนวณค่า $V^{\\pi}(s)$ สำหรับ policy ที่กำหนด:\n",
    "\n",
    "**ความแตกต่างจาก Value Iteration:**\n",
    "- **ไม่มี max operator** เพราะ action ถูกกำหนดโดย policy แล้ว\n",
    "- กลายเป็น **ระบบสมการเชิงเส้น** ที่แก้ได้ง่าย\n",
    "- ใช้ **simplified Bellman update**:\n",
    "\n",
    "$$V_{i+1}(s) = R(s) + \\gamma \\sum_{s'} P(s'|s,\\pi(s))V_i(s')$$\n",
    "\n",
    "**ในโค้ด:**\n",
    "- `action = policy[state]` - ใช้ action ที่ policy กำหนด\n",
    "- ไม่มีการหา max ของ Q-values\n",
    "- ทำการ update แบบ iterative จนกว่าจะ converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged after 3 iterations\n",
      "\n",
      "Final values:\n",
      "   0.812    0.868    0.918    1.000 \n",
      "   0.762    XXXXX    0.660   -1.000 \n",
      "   0.705    0.655    0.611    0.387 \n"
     ]
    }
   ],
   "source": [
    "policy, V, iterations = policy_iteration(grid)\n",
    "\n",
    "print(f\"\\nConverged after {iterations} iterations\")\n",
    "print(\"\\nFinal values:\")\n",
    "for i in range(grid.height):\n",
    "    for j in range(grid.width):\n",
    "        if not grid.is_valid_state((i, j)):\n",
    "            print(\"   XXXXX \", end=\"\")\n",
    "        else:\n",
    "            print(f\" {V[(i, j)]:7.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. สรุปและเปรียบเทียบ Value Iteration vs Policy Iteration\n",
    "\n",
    "### ผลลัพธ์:\n",
    "ทั้งสองอัลกอริทึมจะให้ **optimal policy เดียวกัน** และ **ค่า V เดียวกัน**\n",
    "\n",
    "### ข้อแตกต่าง:\n",
    "\n",
    "| แง่มุม | Value Iteration | Policy Iteration |\n",
    "|--------|----------------|-----------------|\n",
    "| **วิธีการ** | หาค่า V ก่อน แล้วสกัด policy | หา policy โดยตรง |\n",
    "| **ความเร็ว** | อาจช้ากว่า | มักเร็วกว่า |\n",
    "| **ความซับซ้อน** | ง่ายกว่า | ซับซ้อนกว่า |\n",
    "| **การ update** | ทุกสถานะพร้อมกัน | สลับระหว่าง evaluation และ improvement |\n",
    "\n",
    "### การเลือกใช้:\n",
    "- **Value Iteration**: เหมาะสำหรับปัญหาขนาดเล็ก หรือเมื่อต้องการ implementation ที่ง่าย\n",
    "- **Policy Iteration**: เหมาะสำหรับปัญหาขนาดใหญ่ที่ต้องการความเร็ว\n",
    "\n",
    "### Bellman Equation - หัวใจของทั้งสองอัลกอริทึม:\n",
    "$$V(s) = R(s) + \\gamma \\max_a \\sum_{s'} P(s'|s,a) V(s')$$\n",
    "\n",
    "สมการนี้เชื่อมโยงค่าของสถานะปัจจุบันกับค่าของสถานะถัดไป ทำให้เราหา optimal policy ได้"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci193611",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
